{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"loadModelForDeployment.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1slIPejrGAjEqipdAILKRQ-eeiy-V_1Vy","authorship_tag":"ABX9TyMOOdwhRNnhiHZI+9JOYGXx"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FxvUFqBMgRhn","executionInfo":{"status":"ok","timestamp":1604945584467,"user_tz":-330,"elapsed":8650,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}},"outputId":"ce0145d7-8721-463b-ffe3-24348c8e5e2d","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz --quiet"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 14.9MB 1.3MB/s \n","\u001b[?25h  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DI5cl9Rcv3qZ","executionInfo":{"status":"ok","timestamp":1604945585116,"user_tz":-330,"elapsed":9294,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}}},"source":["!cp '/content/drive/My Drive/EVA4/phase2/s11_germanText2EnglishText_Transformers/s11-german2EnglishText-encoder-decoder-full-cpu.pt' .\n","!cp '/content/drive/My Drive/EVA4/phase2/s11_germanText2EnglishText_Transformers/s11-german2EnglishText-encoder-decoder-full-cpu-stdict.pt' .\n","!cp '/content/drive/My Drive/EVA4/phase2/s11_germanText2EnglishText_Transformers/SRC_fields.pkl' .\n","!cp '/content/drive/My Drive/EVA4/phase2/s11_germanText2EnglishText_Transformers/TRG_fields.pkl' ."],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9py_2gjmt4J"},"source":["# Import modules and initial variables"]},{"cell_type":"code","metadata":{"id":"9GPwDxwwxHXW","executionInfo":{"status":"ok","timestamp":1604945585528,"user_tz":-330,"elapsed":9700,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}},"outputId":"9d4e4725-eadb-4f1f-d5d1-20d796086e86","colab":{"base_uri":"https://localhost:8080/"}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","import dill\n","import spacy\n","#from torchtext import data\n","\n","import random,io #,pickle\n","import numpy as np\n","\n","UNK_TOKEN = \"<unk>\"\n","PAD_TOKEN = \"<pad>\"    \n","SOS_TOKEN = \"<s>\"\n","EOS_TOKEN = \"</s>\"\n","LOWER = True\n","\n","DEVICE = 'cpu' #if not torch.cuda.is_available() else 'cuda'\n","\n","print(DEVICE)\n","\n","seed = 1234\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"At6M-Gjnx1hr"},"source":["# Define classes and methods"]},{"cell_type":"code","metadata":{"id":"1wkMdw7BxpW1","executionInfo":{"status":"ok","timestamp":1604945586259,"user_tz":-330,"elapsed":10427,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}}},"source":["class EncoderDecoder(nn.Module):\n","    \"\"\"\n","    A standard Encoder-Decoder architecture. Base for this and many \n","    other models.\n","    \"\"\"\n","    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.trg_embed = trg_embed\n","        self.generator = generator\n","        \n","    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n","        \"\"\"Take in and process masked src and target sequences.\"\"\"\n","        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n","        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n","    \n","    def encode(self, src, src_mask, src_lengths):\n","        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n","    \n","    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n","               decoder_hidden=None):\n","        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n","                            src_mask, trg_mask, hidden=decoder_hidden)\n","        \n","\n","class Generator(nn.Module):\n","    \"\"\"Define standard linear + softmax generation step.\"\"\"\n","    def __init__(self, hidden_size, vocab_size):\n","        super(Generator, self).__init__()\n","        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n","\n","    def forward(self, x):\n","        return F.log_softmax(self.proj(x), dim=-1)\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\"Encodes a sequence of word embeddings\"\"\"\n","    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n","        super(Encoder, self).__init__()\n","        self.num_layers = num_layers\n","        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n","                          batch_first=True, bidirectional=True, dropout=dropout)\n","        \n","    def forward(self, x, mask, lengths):\n","        \"\"\"\n","        Applies a bidirectional GRU to sequence of embeddings x.\n","        The input mini-batch x needs to be sorted by length.\n","        x should have dimensions [batch, time, dim].\n","        \"\"\"\n","        packed = pack_padded_sequence(x, lengths, batch_first=True)\n","        output, final = self.rnn(packed)\n","        output, _ = pad_packed_sequence(output, batch_first=True)\n","\n","        # we need to manually concatenate the final states for both directions\n","        fwd_final = final[0:final.size(0):2]\n","        bwd_final = final[1:final.size(0):2]\n","        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n","\n","        return output, final\n","\n","\n","class Decoder(nn.Module):\n","    \"\"\"A conditional RNN decoder with attention.\"\"\"\n","    \n","    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n","                 bridge=True):\n","        super(Decoder, self).__init__()\n","        \n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.attention = attention\n","        self.dropout = dropout\n","                 \n","        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n","                          batch_first=True, dropout=dropout)\n","                 \n","        # to initialize from the final encoder state\n","        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n","\n","        self.dropout_layer = nn.Dropout(p=dropout)\n","        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n","                                          hidden_size, bias=False)\n","        \n","    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n","        \"\"\"Perform a single decoder step (1 word)\"\"\"\n","\n","        # compute context vector using attention mechanism\n","        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n","        context, attn_probs = self.attention(\n","            query=query, proj_key=proj_key,\n","            value=encoder_hidden, mask=src_mask)\n","\n","        # update rnn hidden state\n","        rnn_input = torch.cat([prev_embed, context], dim=2)\n","        output, hidden = self.rnn(rnn_input, hidden)\n","        \n","        pre_output = torch.cat([prev_embed, output, context], dim=2)\n","        pre_output = self.dropout_layer(pre_output)\n","        pre_output = self.pre_output_layer(pre_output)\n","\n","        return output, hidden, pre_output\n","    \n","    def forward(self, trg_embed, encoder_hidden, encoder_final, \n","                src_mask, trg_mask, hidden=None, max_len=None):\n","        \"\"\"Unroll the decoder one step at a time.\"\"\"\n","                                         \n","        # the maximum number of steps to unroll the RNN\n","        if max_len is None:\n","            max_len = trg_mask.size(-1)\n","\n","        # initialize decoder hidden state\n","        if hidden is None:\n","            hidden = self.init_hidden(encoder_final)\n","        \n","        # pre-compute projected encoder hidden states\n","        # (the \"keys\" for the attention mechanism)\n","        # this is only done for efficiency\n","        proj_key = self.attention.key_layer(encoder_hidden)\n","        \n","        # here we store all intermediate hidden states and pre-output vectors\n","        decoder_states = []\n","        pre_output_vectors = []\n","        \n","        # unroll the decoder RNN for max_len steps\n","        for i in range(max_len):\n","            prev_embed = trg_embed[:, i].unsqueeze(1)\n","            output, hidden, pre_output = self.forward_step(\n","              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n","            decoder_states.append(output)\n","            pre_output_vectors.append(pre_output)\n","\n","        decoder_states = torch.cat(decoder_states, dim=1)\n","        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n","        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n","\n","    def init_hidden(self, encoder_final):\n","        \"\"\"Returns the initial decoder state,\n","        conditioned on the final encoder state.\"\"\"\n","\n","        if encoder_final is None:\n","            return None  # start with zeros\n","\n","        return torch.tanh(self.bridge(encoder_final))            \n","\n","\n","class BahdanauAttention(nn.Module):\n","    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n","    \n","    def __init__(self, hidden_size, key_size=None, query_size=None):\n","        super(BahdanauAttention, self).__init__()\n","        \n","        # We assume a bi-directional encoder so key_size is 2*hidden_size\n","        key_size = 2 * hidden_size if key_size is None else key_size\n","        query_size = hidden_size if query_size is None else query_size\n","\n","        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","        \n","        # to store attention scores\n","        self.alphas = None\n","        \n","    def forward(self, query=None, proj_key=None, value=None, mask=None):\n","        assert mask is not None, \"mask is required\"\n","\n","        # We first project the query (the decoder state).\n","        # The projected keys (the encoder states) were already pre-computated.\n","        query = self.query_layer(query)\n","        \n","        # Calculate scores.\n","        scores = self.energy_layer(torch.tanh(query + proj_key))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","        \n","        # Mask out invalid positions.\n","        # The mask marks valid positions so we invert it using `mask & 0`.\n","        scores.data.masked_fill_(mask == 0, -float('inf'))\n","        \n","        # Turn scores to probabilities.\n","        alphas = F.softmax(scores, dim=-1)\n","        self.alphas = alphas        \n","        \n","        # The context vector is the weighted sum of the values.\n","        context = torch.bmm(alphas, value)\n","        \n","        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n","        return context, alphas"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6gnCMokxtEc","executionInfo":{"status":"ok","timestamp":1604945586260,"user_tz":-330,"elapsed":10425,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}}},"source":["def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n","    \"\"\"Greedily decode a sentence.\"\"\"\n","\n","    with torch.no_grad():\n","        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n","        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n","        trg_mask = torch.ones_like(prev_y)\n","\n","    output = []\n","    attention_scores = []\n","    hidden = None\n","\n","    for i in range(max_len):\n","        with torch.no_grad():\n","            out, hidden, pre_output = model.decode(\n","              encoder_hidden, encoder_final, src_mask,\n","              prev_y, trg_mask, hidden)\n","\n","            # we predict from the pre-output layer, which is\n","            # a combination of Decoder state, prev emb, and context\n","            prob = model.generator(pre_output[:, -1])\n","\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.data.item()\n","        output.append(next_word)\n","        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n","        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n","    \n","    output = np.array(output)\n","        \n","    # cut off everything starting from </s> \n","    # (only when eos_index provided)\n","    if eos_index is not None:\n","        first_eos = np.where(output==eos_index)[0]\n","        if len(first_eos) > 0:\n","            output = output[:first_eos[0]]      \n","    \n","    return output, np.concatenate(attention_scores, axis=1)\n","  \n","def lookup_words(x, vocab=None):\n","    if vocab is not None:\n","        x = [vocab.itos[i] for i in x]\n","\n","    return [str(t) for t in x]\n","\n","def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n","    \"Helper: Construct a model from hyperparameters.\"\n","\n","    attention = BahdanauAttention(hidden_size)\n","\n","    model = EncoderDecoder(\n","        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n","        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n","        nn.Embedding(src_vocab, emb_size),\n","        nn.Embedding(tgt_vocab, emb_size),\n","        Generator(hidden_size, tgt_vocab))\n","\n","    return model.cuda() if torch.cuda.is_available() else model"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fewJNggFW4Ow"},"source":["# Load model"]},{"cell_type":"code","metadata":{"id":"pD-w3n-8k6Bw","executionInfo":{"status":"ok","timestamp":1604945586262,"user_tz":-330,"elapsed":10422,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}}},"source":["with open(\"/content/SRC_fields.pkl\",'rb') as f:\n","    bytesstream_SRC = io.BytesIO(f.read())\n","\n","with open(\"/content/TRG_fields.pkl\",'rb') as f:\n","    bytesstream_TRG = io.BytesIO(f.read())\n","\n","SRC = torch.load(bytesstream_SRC, pickle_module=dill)\n","TRG = torch.load(bytesstream_TRG, pickle_module=dill)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vr5gb-oYK0t4","executionInfo":{"status":"ok","timestamp":1604945586262,"user_tz":-330,"elapsed":10419,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}},"outputId":"c8e3faab-fcad-4e39-cadb-86b0dc52fe09","colab":{"base_uri":"https://localhost:8080/"}},"source":["(len(SRC.vocab), len(TRG.vocab))\n","                   #emb_size=256, hidden_size=256,\n","                   #num_layers=1, dropout=0.2)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15765, 13002)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"9CkFXg7yyNFX","executionInfo":{"status":"ok","timestamp":1604945586265,"user_tz":-330,"elapsed":10418,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}},"outputId":"dfa00325-f3a5-4015-ef1e-915acef30eb9","colab":{"base_uri":"https://localhost:8080/"}},"source":["#num_words = 11\n","criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=0)\n","#model = make_model(num_words, num_words, emb_size=32, hidden_size=64)\n","model = make_model(len(SRC.vocab), len(TRG.vocab),\n","                   emb_size=256, hidden_size=256,\n","                   num_layers=1, dropout=0.2)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eT199BruMsQ3","executionInfo":{"status":"ok","timestamp":1604945586266,"user_tz":-330,"elapsed":10416,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHKdSnY5yuPT","executionInfo":{"status":"ok","timestamp":1604945586782,"user_tz":-330,"elapsed":10929,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}},"outputId":"e8ad5a94-9439-4ba7-ed77-bd04e6c329f7","colab":{"base_uri":"https://localhost:8080/"}},"source":["model.eval()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (rnn): GRU(256, 256, batch_first=True, dropout=0.2, bidirectional=True)\n","  )\n","  (decoder): Decoder(\n","    (attention): BahdanauAttention(\n","      (key_layer): Linear(in_features=512, out_features=256, bias=False)\n","      (query_layer): Linear(in_features=256, out_features=256, bias=False)\n","      (energy_layer): Linear(in_features=256, out_features=1, bias=False)\n","    )\n","    (rnn): GRU(768, 256, batch_first=True, dropout=0.2)\n","    (bridge): Linear(in_features=512, out_features=256, bias=True)\n","    (dropout_layer): Dropout(p=0.2, inplace=False)\n","    (pre_output_layer): Linear(in_features=1024, out_features=256, bias=False)\n","  )\n","  (src_embed): Embedding(15765, 256)\n","  (trg_embed): Embedding(13002, 256)\n","  (generator): Generator(\n","    (proj): Linear(in_features=256, out_features=13002, bias=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"M3DD-xBQy8_z","executionInfo":{"status":"ok","timestamp":1604945586788,"user_tz":-330,"elapsed":10931,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}}},"source":["#with open('/content/s11-german2EnglishText-encoder-decoder-full-cpu.pt','rb') as f:\n","with open('/content/s11-german2EnglishText-encoder-decoder-full-cpu-stdict.pt','rb') as f:\n","    bytesstream_model = io.BytesIO(f.read())\n","chkpnt = torch.load(bytesstream_model,map_location=torch.device('cpu'))\n","\n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFDrJR7TMxFH","executionInfo":{"status":"ok","timestamp":1604945586791,"user_tz":-330,"elapsed":10930,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}},"outputId":"00ad1f8d-802e-49b6-d5fd-c43659b92057","colab":{"base_uri":"https://localhost:8080/"}},"source":["model.load_state_dict(chkpnt)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"PhXpeME5zoEi"},"source":["# Prediction"]},{"cell_type":"code","metadata":{"id":"6fWmmz3Pzd5Z","executionInfo":{"status":"ok","timestamp":1604945586792,"user_tz":-330,"elapsed":10928,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}}},"source":["# Make Prediction\n","\n","def translate_german2english(model, sentence,spacy_model,src_fields,DEVICE='cpu'):\n","    model.eval()\n","    tokenized = [tok.text for tok in spacy_model.tokenizer(sentence)]\n","    print(tokenized)\n","    indexed = [src_fields.vocab.stoi[t] for t in tokenized]\n","    print(indexed)\n","    tensor = torch.LongTensor(indexed).to(DEVICE)\n","    tensor = tensor.unsqueeze(0)\n","    print(tensor)\n","    print(tensor.size())\n","    pad_index = 0\n","    src_mask = (tensor != pad_index).unsqueeze(0).to(DEVICE)\n","    print(src_mask)\n","    print(src_mask.size())\n","    src_lengths = torch.LongTensor([tensor.size()[1]]).to(DEVICE)\n","    print(src_lengths)\n","    print(src_lengths.size())\n","    result, _ = greedy_decode(\n","          model, tensor, src_mask, src_lengths,\n","          max_len=25, sos_index=TRG.vocab.stoi[SOS_TOKEN], eos_index=TRG.vocab.stoi[EOS_TOKEN])\n","    \n","    output = lookup_words(result,vocab= TRG.vocab)\n","    outText = ' '.join(output)\n","        \n","    return outText"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZn6mKBtLt7c","executionInfo":{"status":"ok","timestamp":1604945586794,"user_tz":-330,"elapsed":10926,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}},"outputId":"f61e8c41-62e8-42f4-9c09-785a07248e81","colab":{"base_uri":"https://localhost:8080/"}},"source":["len(SRC.vocab)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15765"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"bwLkmL2_zwAC","executionInfo":{"status":"ok","timestamp":1604945588345,"user_tz":-330,"elapsed":12474,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}},"outputId":"8ad5b412-5c89-4ef4-a405-7a2cad0b324f","colab":{"base_uri":"https://localhost:8080/","height":213}},"source":["import de_core_news_sm\n","\n","spacy_de = spacy.load('de')\n","inText = \"als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\"\n","\n","#result = \n","# chkpnt\n","translate_german2english(model,inText,spacy_de,SRC)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["['als', 'ich', '11', 'jahre', 'alt', 'war', ',', 'wurde', 'ich', 'eines', 'morgens', 'von', 'den', '<', 'unk', '>', 'heller', 'freude', 'geweckt', '.']\n","[41, 9, 1012, 144, 464, 35, 4, 84, 9, 126, 1715, 21, 27, 0, 0, 0, 11351, 1117, 8043, 3]\n","tensor([[   41,     9,  1012,   144,   464,    35,     4,    84,     9,   126,\n","          1715,    21,    27,     0,     0,     0, 11351,  1117,  8043,     3]])\n","torch.Size([1, 20])\n","tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n","           True,  True,  True, False, False, False,  True,  True,  True,  True]]])\n","torch.Size([1, 1, 20])\n","tensor([20])\n","torch.Size([1])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'when i was 11 , i was claimed to the morning from the morning of my joy , joy . jy . jy .'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"PUOhbck4V9H3","executionInfo":{"status":"ok","timestamp":1604945588348,"user_tz":-330,"elapsed":12471,"user":{"displayName":"vamsi suman","photoUrl":"","userId":"08791886420097491048"}}},"source":[""],"execution_count":14,"outputs":[]}]}